{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "def relu_derivative(x):\n",
    "    return (x>0).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_x=np.exp(x-np.max(x,axis=1,keepdims=True))\n",
    "    return exp_x/np.sum(exp_x,axis=1,keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_loss(y_true,y_pred):\n",
    "    return -np.mean(np.sum(y_true*np.log(y_pred+1e-8),axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x,w1,b1,w2,b2):\n",
    "    hidden_layer=relu(np.dot(x,w1)+b1)\n",
    "    output_layer=np.dot(hidden_layer,w2)+b2\n",
    "    y_pred=softmax(output_layer)\n",
    "    return hidden_layer,y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(x,y,hidden_layer,y_pred,w2):\n",
    "    grad_output=(y_pred-y)/y.shape[0]\n",
    "    grad_w2=np.dot(hidden_layer.T,grad_output)\n",
    "    grad_b2=np.sum(grad_output,axis=0,keepdims=True)\n",
    "    grad_hidden=np.dot(grad_output,w2.T)*(hidden_layer>0)\n",
    "    grad_w1=np.dot(x.T,grad_hidden)\n",
    "    grad_b1=np.sum(grad_hidden,axis=0,keepdims=True)\n",
    "    return grad_w1,grad_b1,grad_w2,grad_b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x,y,inputsize,hiddensize,outputsize,epochs=500,learning_rate=0.001):\n",
    "    w1=np.random.randn(inputsize,hiddensize)*0.01\n",
    "    b1=np.zeros((1,hiddensize))\n",
    "    w2=np.random.randn(hiddensize,outputsize)*0.01\n",
    "    b2=np.zeros((1,outputsize))\n",
    "    for epoch in range(epochs):\n",
    "        hidden_layer,y_pred=forward(x,w1,b1,w2,b2)\n",
    "        loss=entropy_loss(y,y_pred)\n",
    "        grad_w1,grad_b1,grad_w2,grad_b2=backward(x,y,hidden_layer,y_pred,w2)\n",
    "        w1-=learning_rate*grad_w1\n",
    "        b1-=learning_rate*grad_b1\n",
    "        w2-=learning_rate*grad_w2\n",
    "        b2-=learning_rate*grad_b2\n",
    "        if(epoch+1)%100==0:\n",
    "            print(f\"Epoch{epoch+1}/{epochs},Loss:{loss:4f}\")\n",
    "        return w1,b1,w2,b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x,w1,b1,w2,b2):\n",
    "    hidden_layer=relu(np.dot(x,w1)+b1)\n",
    "    output_layer=np.dot(hidden_layer,w2)+b2\n",
    "    probabilities=softmax(output_layer)\n",
    "    return np.argmax(probabilities,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(folder_path,imagesize=(28,28),return_labels=True):\n",
    "    images=[]\n",
    "    labels=[]\n",
    "    file_paths={}\n",
    "    label_map={}\n",
    "    label_index=0\n",
    "    for label in os.listdir(folder_path):\n",
    "        label_path=os.path.join(folder_path,label)\n",
    "        if os.path.isdir(label_path):\n",
    "            if label not in label_map:\n",
    "                label_map[label]=label_index\n",
    "                label_index+=1\n",
    "            for file_name in os.listdir(label_path):\n",
    "                img_path=os.path.join(label_path, file_name)\n",
    "                img=Image.open(img_path).convert('L').resize(imgsize)\n",
    "                images.append(np.array(img).flatten())\n",
    "                labels.append(label_map[label])\n",
    "                file_paths.append(img_path)\n",
    "    if return_labels:\n",
    "        return np.array(images),np.array(labels),file_paths,label_map\n",
    "    else:\n",
    "        return np.array(images),file_paths\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(true_labels,predicted_labels):\n",
    "    correct=np.sum(true_labels==predicted_labels)\n",
    "    total=len(true_labels)\n",
    "    accuracy=(correct/total)*100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'C:\\\\Users\\\\mahek\\\\mahek\\\\.ipynb_checkpoints\\\\Train\\\\Train\\\\Jade'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m test_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Load training data\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m X_train, y_train, _, label_map \u001b[38;5;241m=\u001b[39m \u001b[43mload_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Load testing data\u001b[39;00m\n\u001b[0;32m      9\u001b[0m X_test, test_labels, test_files, _ \u001b[38;5;241m=\u001b[39m load_images(test_path)\n",
      "Cell \u001b[1;32mIn[66], line 15\u001b[0m, in \u001b[0;36mload_images\u001b[1;34m(folder_path, imagesize, return_labels)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(label_path):\n\u001b[0;32m     14\u001b[0m     img_path\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(label_path, file_name)\n\u001b[1;32m---> 15\u001b[0m     img\u001b[38;5;241m=\u001b[39m\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mresize(imgsize)\n\u001b[0;32m     16\u001b[0m     images\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39marray(img)\u001b[38;5;241m.\u001b[39mflatten())\n\u001b[0;32m     17\u001b[0m     labels\u001b[38;5;241m.\u001b[39mappend(label_map[label])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\PIL\\Image.py:3431\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3428\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(os\u001b[38;5;241m.\u001b[39mfspath(fp))\n\u001b[0;32m   3430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m-> 3431\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3432\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'C:\\\\Users\\\\mahek\\\\mahek\\\\.ipynb_checkpoints\\\\Train\\\\Train\\\\Jade'"
     ]
    }
   ],
   "source": [
    "# Paths to datasets\n",
    "train_path = \"Train\"\n",
    "test_path = \"Test\"\n",
    "    \n",
    "# Load training data\n",
    "X_train, y_train, _, label_map = load_images(train_path)\n",
    "    \n",
    "# Load testing data\n",
    "X_test, test_labels, test_files, _ = load_images(test_path)\n",
    "    \n",
    "# Normalize the datasets\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "    \n",
    "# One-hot encode training labels\n",
    "num_classes = len(label_map)\n",
    "y_train_one_hot = np.zeros((y_train.size, num_classes))\n",
    "y_train_one_hot[np.arange(y_train.size), y_train] = 1\n",
    "    \n",
    "# Train the neural network\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 64\n",
    "output_size = num_classes\n",
    "w1, b1, w2, b2 = train(X_train, y_train_one_hot, input_size, hidden_size, output_size)\n",
    "    \n",
    "# Predict classes for Tim's Kins\n",
    "predictions = predict(X_test, w1, b1, w2, b2)\n",
    "    \n",
    "# Create a reverse map for class labels\n",
    "reverse_label_map = {v: k for k, v in label_map.items()}\n",
    "    \n",
    "# Print predictions with corresponding file names and class labels\n",
    "print(\"\\nPredictions for Tim's Kins:\")\n",
    "for i in range(len(predictions)):\n",
    "    true_label = reverse_label_map[test_labels[i]]\n",
    "    predicted_label = reverse_label_map[predictions[i]]\n",
    "    print(f\"Image: {os.path.basename(test_files[i])}, True Class: {true_label}, Predicted Class: {predicted_label}\")\n",
    "\n",
    "accuracy = calculate_accuracy(test_labels, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
